{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a8e4d00",
   "metadata": {},
   "source": [
    "# READ DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5bfb4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Load the dictionary from file\n",
    "dictionary = Dictionary.load('dictionary')\n",
    "print('Dictionary length:', len(dictionary), '\\n')\n",
    "\n",
    "import json\n",
    "\n",
    "# Load data from reference_sheet.json\n",
    "with open('reference_sheet.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "reference_sheet = json.loads(json_data)\n",
    "\n",
    "# Load data from topic_reference_sheet.json\n",
    "with open('topic_reference_sheet.json', 'r') as file:\n",
    "    json_data = file.read()\n",
    "topic_reference_sheet = json.loads(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b3e528",
   "metadata": {},
   "source": [
    "# READ CORPUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bd16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "import ast\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set the path to the original data folder\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(os.path.join(path_original_data, 'post_processed_comment_data_demo.csv'), low_memory=False)\n",
    "\n",
    "# Extract the 'gensim_comment' column as a list of strings\n",
    "string_list = df['gensim_comment'].tolist()\n",
    "\n",
    "# Convert the string representation of lists to actual lists\n",
    "comments_list = [ast.literal_eval(s) for s in string_list]\n",
    "\n",
    "# Convert the comments to bag-of-words representation using the loaded dictionary\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in comments_list]\n",
    "print('Length of the corpus:', len(bow_corpus), '\\n')\n",
    "\n",
    "# Choose a random document from the corpus\n",
    "random_number = round(random.uniform(0, len(bow_corpus)))\n",
    "bow_doc_x = bow_corpus[random_number]\n",
    "\n",
    "# Print the word count for each word in the chosen document\n",
    "for i in range(len(bow_doc_x)):\n",
    "    print(\"Word {} ('{}') appears {} time.\".format(bow_doc_x[i][0], dictionary[bow_doc_x[i][0]], bow_doc_x[i][1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dab6ee",
   "metadata": {},
   "source": [
    "# TRAIN MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9976acd",
   "metadata": {},
   "source": [
    "### IMPORTANT VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5eb40643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER OF TOPICS\n",
    "topic_num = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478438f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim import models\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Create an instance of LdaMulticore model\n",
    "lda = gensim.models.LdaMulticore(\n",
    "    corpus=bow_corpus,    # The bag-of-words corpus\n",
    "    num_topics=topic_num, # Number of topics to generate\n",
    "    id2word=dictionary,   # Mapping of word IDs to words\n",
    "    passes=50,            # Number of passes through the corpus\n",
    "    workers=2             # Number of worker processes\n",
    ")\n",
    "\n",
    "# Alternative approach using LdaModel\n",
    "# lda = gensim.models.LdaModel(\n",
    "#     corpus=bow_corpus,\n",
    "#     num_topics=topic_num,\n",
    "#     id2word=dictionary,\n",
    "#     passes=50\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3446994",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gensim import models\n",
    "\n",
    "# Set the path to the original data folder\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "\n",
    "# Create the path for the temporary file\n",
    "temp_file = os.path.join(path_original_data, 'lda_model')\n",
    "\n",
    "# Save the LdaModel to the temporary file\n",
    "# lda.save(temp_file)\n",
    "\n",
    "# Load the LdaModel from the temporary file\n",
    "lda = models.ldamodel.LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "079657c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topics_read = list(range(0, topic_num))\n",
    "topic_dict = {}\n",
    "\n",
    "# Loop until all topics have been read\n",
    "while len(topics_read) > 0:\n",
    "    # Iterate over the topics\n",
    "    for topic in lda.print_topics():\n",
    "        topic_id, topic_words = topic\n",
    "        # Check if the current topic is in the unread topics list\n",
    "        if topic_id in topics_read:\n",
    "            topics_read.remove(topic_id)\n",
    "            words = topic_words.split(' + ')\n",
    "            topic_dict[topic_id] = {}\n",
    "            # Process each word in the topic\n",
    "            for i, word in enumerate(words):\n",
    "                word = word.split('\"')\n",
    "                # Check if the word is in the reference_sheet\n",
    "                if word[1] in reference_sheet:\n",
    "                    topic_dict[topic_id][word[1]] = (word[0].replace('*', ''), reference_sheet[word[1]])\n",
    "                else:\n",
    "                    topic_dict[topic_id][word[1]] = (word[0].replace('*', ''), [word[1].upper()])\n",
    "    print('- Still unread:', topics_read)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a0a718",
   "metadata": {},
   "source": [
    "# RESULTING TOPICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1262f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def find_closest_match(input_string, string_list):\n",
    "    highest_ratio = 0\n",
    "    closest_string = None\n",
    "\n",
    "    # Iterate over the string list\n",
    "    for string in string_list:\n",
    "        # Calculate the fuzz ratio between the input string and the current string\n",
    "        ratio = fuzz.ratio(input_string, string)\n",
    "        # Update the highest ratio and closest string if the current ratio is higher\n",
    "        if ratio > highest_ratio:\n",
    "            highest_ratio = ratio\n",
    "            closest_string = string\n",
    "\n",
    "    return closest_string\n",
    "\n",
    "# Sort the topic_dict by topic key in ascending order\n",
    "ordered_dict = dict(sorted(topic_dict.items(), key=lambda x: int(x[0])))\n",
    "\n",
    "topic_assignment = {}\n",
    "\n",
    "# Iterate over the ordered_dict\n",
    "for topic_key, topic_value in ordered_dict.items():\n",
    "    print('TOPIC:', topic_key)\n",
    "    \n",
    "    # Initialize the topic assignment dictionary for the current topic\n",
    "    topic_assignment[topic_key] = {}\n",
    "    \n",
    "    # Iterate over the words and probabilities in the current topic\n",
    "    for key, value in topic_value.items():\n",
    "        wprob, wlist = value\n",
    "        \n",
    "        word_list = []\n",
    "        \n",
    "        # Split each string in wlist and add the words to word_list\n",
    "        for string in wlist:\n",
    "            words = string.split()\n",
    "            word_list.extend(words)\n",
    "        \n",
    "        # Find the closest match for the key in the word_list\n",
    "        closest_match = find_closest_match(key, word_list)\n",
    "        \n",
    "        # If no closest match is found, set it to the shortest string in wlist with an asterisk\n",
    "        if closest_match is None:\n",
    "            closest_match = min(wlist, key=len) + '*'\n",
    "        \n",
    "        # If the closest match is in uppercase, set it to '-'\n",
    "        if closest_match.isupper():\n",
    "            closest_match = '-'\n",
    "        \n",
    "        # Print the word and the closest match\n",
    "        print(key, ':', closest_match.lower())\n",
    "        \n",
    "        # Check if the word is in the topic_reference_sheet\n",
    "        if key in topic_reference_sheet:\n",
    "            # Assign the corresponding value to topic_origin in topic_assignment\n",
    "            topic_assignment[topic_key][key] = topic_reference_sheet[key]\n",
    "            \n",
    "            # If there are multiple topic origins, join them with commas\n",
    "            if len(topic_reference_sheet[key]) > 1:\n",
    "                topic_origin = ', '.join(topic_reference_sheet[key])\n",
    "            else:\n",
    "                topic_origin = topic_reference_sheet[key][0]\n",
    "        else:\n",
    "            # If the word is not in the topic_reference_sheet, set topic_origin to '-'\n",
    "            topic_origin = '-'\n",
    "            \n",
    "            # Store the topic_origin in topic_assignment\n",
    "            topic_assignment[topic_key][key] = topic_origin\n",
    "        \n",
    "        # Print the word and its corresponding topic origin\n",
    "        print('*', key, ':', topic_origin)\n",
    "    \n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb7fa8",
   "metadata": {},
   "source": [
    "# LABEL TOPICS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81067369",
   "metadata": {},
   "source": [
    "#### WARNING\n",
    "The names written may not be always representive of the topics at the current moment since even without major changes in content the order they are shown can change.\n",
    "\n",
    "Check before using the manually assigned names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e35609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = {0: \"Actions\", 1: \"NA\", 2: \"NA\", 3: \"Learning\", 4: \"Game design\", 5: \"Number of players\",\n",
    "              6: \"Number of players\", 7: \"Game duration\", 8: \"Rules\", 9: \"Problem-solving\", 10: \"Release\",\n",
    "              11: \"Gameplay\", 12: \"Concept\", 13: \"Scoring\", 14: \"Artwork\", 15: \"D&D\", 16: \"Strategy\",\n",
    "              17: \"Recommendation\", 18: \"Favourite\", 19: \"Mechanics\", 20: \"Actions\", 21: \"Favourite\",\n",
    "              22: \"NA\", 23: \"D&D\", 24: \"Interaction\", 25: \"Version\", 26: \"Luck\", 27: \"Winning\",\n",
    "              28: \"Length\", 29: \"Components\"}\n",
    "\n",
    "my_topics = {\"bookeeping\": [28, 8], \"downtime\": [24], \"interaction\": [4, 9], \"bash the leader\": [7, 13, 27],\n",
    "             \"complex or complicated\": [3, 16, 20, 0], \"luck\": [26]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba65634",
   "metadata": {},
   "source": [
    "### IMPORTANT VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f4dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NUMBER OF TIMES A CATEGORY HAS TO APPEAR IN A TOPIC TO BE CONSIDERED AS THE TOPIC\n",
    "topic_bound = 4\n",
    "\n",
    "# THE MAX DIFFERENCE BETWEEN TO ELEMENTS TO CONSIDERED JOINING THEM\n",
    "union_bound = 2\n",
    "\n",
    "# If the difference is too big that means one of the categories can stand on its own, if it's less then combining may boost its probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a813d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def return_draw(my_dict):\n",
    "    # Retrieve the first and second elements from the dictionary\n",
    "    first_element = list(my_dict.keys())[0]\n",
    "    second_element = list(my_dict.keys())[1]\n",
    "    # Retrieve the values corresponding to the first and second elements\n",
    "    first_value = my_dict[first_element]\n",
    "    second_value = my_dict[second_element]\n",
    "    \n",
    "    # Check if the values are equal\n",
    "    if first_value == second_value:\n",
    "        # If equal, find all keys with the same value\n",
    "        matching_keys = [key for key, value in my_dict.items() if value == first_value]\n",
    "    else:\n",
    "        # If not equal, consider only the first element\n",
    "        matching_keys = [first_element]\n",
    "\n",
    "    return matching_keys\n",
    "\n",
    "topic_estimation = {}\n",
    "topic_classification = {}\n",
    "\n",
    "# Iterate over the topic_assignment dictionary\n",
    "for topic_key, topic_value in topic_assignment.items():\n",
    "    topic_count = {}\n",
    "    # Iterate over the values in the topic_value dictionary\n",
    "    for key, value in topic_value.items():\n",
    "        # Iterate over each topic in the value list\n",
    "        for topic in value:\n",
    "            # Increment the count for each topic in the topic_count dictionary\n",
    "            current = topic_count.setdefault(topic, 0)\n",
    "            topic_count[topic] = current + 1\n",
    "    \n",
    "    # Sort the topic_count dictionary by count in descending order\n",
    "    sorted_dict = dict(sorted(topic_count.items(), key=lambda x: x[1], reverse=True))\n",
    "    # Remove the '-' key from the sorted_dict if present\n",
    "    if '-' in sorted_dict:\n",
    "        del sorted_dict['-']\n",
    "    \n",
    "    # Assign the sorted_dict to the corresponding topic key in topic_estimation\n",
    "    topic_estimation[topic_key] = sorted_dict        \n",
    "\n",
    "# Iterate over the topic_estimation dictionary\n",
    "for key, value in topic_estimation.items():\n",
    "    key_to_fuse1 = 'complex'\n",
    "    key_to_fuse2 = 'complicated'\n",
    "    # Retrieve the values for key_to_fuse1 and key_to_fuse2 from the value dictionary\n",
    "    value1 = value.setdefault(key_to_fuse1, 0)\n",
    "    value2 = value.setdefault(key_to_fuse2, 0)\n",
    "    # Calculate the sum of the values\n",
    "    sum_value = value1 + value2\n",
    "    # Update the value dictionary with the fused key and sum value\n",
    "    value[\"complex or complicated\"] = sum_value\n",
    "    # Remove the individual keys from the value dictionary\n",
    "    del value[key_to_fuse1]\n",
    "    del value[key_to_fuse2]\n",
    "    \n",
    "    # Additional fusion example (commented out)\n",
    "    # key_to_fuse1 = 'downtime'\n",
    "    # key_to_fuse2 = 'interaction'\n",
    "    # value1 = value.setdefault(key_to_fuse1, 0)\n",
    "    # value2 = value.setdefault(key_to_fuse2, 0)\n",
    "    # if abs(value1 - value2) <= union_bound:\n",
    "    #     sum_value = value1 + value2\n",
    "    #     value[\"game dynamics\"] = sum_value\n",
    "    #     del value[key_to_fuse1]\n",
    "    #     del value[key_to_fuse2]\n",
    "    \n",
    "    # Sort the value dictionary by count in descending order\n",
    "    value = dict(sorted(value.items(), key=lambda x: x[1], reverse=True))\n",
    "    # Update the value in the topic_estimation dictionary\n",
    "    topic_estimation[key] = value\n",
    "\n",
    "# Iterate over the topic_estimation dictionary\n",
    "for key, value in topic_estimation.items():\n",
    "    # Retrieve the first element from the value dictionary\n",
    "    first_element = next(iter(value.items()))\n",
    "    name, count = first_element\n",
    "    if count >= topic_bound:\n",
    "        if len(return_draw(value)) == 1:\n",
    "            # If only one key is returned, assign it as the topic classification\n",
    "            topic_classification[key] = [name]\n",
    "        else:\n",
    "            # Otherwise, assign the returned keys as the topic classification\n",
    "            topic_classification[key] = return_draw(value)\n",
    "    else:\n",
    "        # If the count is below the topic_bound, assign 'NA' as the topic classification\n",
    "        topic_classification[key] = ['NA']\n",
    "        \n",
    "# Print the topic key and its corresponding topic classification\n",
    "for key, value in topic_classification.items():       \n",
    "    print(key, '-', value)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ff9305",
   "metadata": {},
   "source": [
    "# EXAMINE COMMENTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6fe68",
   "metadata": {},
   "source": [
    "### IMPORTANT VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae23eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After many iteration the next code is quite convoluded. These are the variables need to change it function.\n",
    "\n",
    "# NUMBER OF MATCHES WITH THE DICTIONARY NEEDED TO TRUST THE RESULT\n",
    "# Extremely short messages are prone to have phony results\n",
    "bound = 5\n",
    "\n",
    "# MINIMUM PROBABILITY NEEDED TO STORE THE TOPIC DETECTED\n",
    "# Some comments barely touch some topics, this helps to avoid long list of topics\n",
    "bound_prob = 0.05\n",
    "\n",
    "# BOOLEAN - USE THE AUTOMATICALLY LABELLED TOPICS\n",
    "# Highest priority, overrides the rest of variables\n",
    "bool_auto = True\n",
    "\n",
    "# BOOLEAN - USE THE MANUALLY ASSIGNED INDIVIDUAL TOPIC LABELS INSTEAD THE GENERAL TOPIC LABELS\n",
    "# Second highest priority, overrides the next variable.\n",
    "bool_title = False\n",
    "\n",
    "# BOOLEAN - GROUP ALL MANUALLY ASIGNED GENERAL TOPIC LABELS SO THERE ARE NO DUPLICATES\n",
    "# Lowest priority, if all is false will use the manual general labels with duplicates.\n",
    "bool_group = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf6f51b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Function to print the comment and return the document bag-of-words representation\n",
    "def print_comment(n):\n",
    "    target_bow = bow_corpus[n]\n",
    "    index = next((i for i, bow_element in enumerate(bow_corpus) if bow_element == target_bow), None)\n",
    "\n",
    "    if index is not None:\n",
    "        print(\"Index of the element in `comments_list`:\", index, '\\n')\n",
    "    else:\n",
    "        print(\"Element not found in `comments_list`.\", '\\n')\n",
    "\n",
    "    document_bow = bow_corpus[index]\n",
    "    document_original = \" \".join([dictionary[id] for id, _ in document_bow])\n",
    "\n",
    "    if len(document_original.split()) <= bound:\n",
    "        print('- WARNING: Amount of data insufficient.', '\\n')\n",
    "\n",
    "    element = comments_list[index]\n",
    "    comment = df.loc[index, 'comment']\n",
    "\n",
    "    print(comment, '\\n')\n",
    "\n",
    "    return document_bow\n",
    "\n",
    "# Function to find keys in a dictionary with a given value\n",
    "def find_keys_with_value(dictionary, number):\n",
    "    return [key for key, values in dictionary.items() if number in values]\n",
    "\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import operator\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "random_number = round(random.uniform(0, len(bow_corpus)))\n",
    "\n",
    "# Get the topics for a randomly selected document and sort them by probability\n",
    "document_topics = lda.get_document_topics(print_comment(random_number))\n",
    "sorted_topics = sorted(document_topics, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "topics_detected = []\n",
    "high_topics_detected = {}\n",
    "for topic_id, topic_prob in sorted_topics:\n",
    "    percentage = topic_prob * 100\n",
    "    formatted_percentage = \"{:.2f}%\".format(percentage)\n",
    "    if topic_prob >= bound_prob:\n",
    "        if bool_auto:\n",
    "            # If bool_auto is True, accumulate the topic probabilities for each topic\n",
    "            for topic in topic_classification[topic_id]:\n",
    "                high_topics_detected[topic] = high_topics_detected.get(topic, 0) + percentage\n",
    "        else:\n",
    "            if bool_title:\n",
    "                # If bool_title is True, include topic names in the detected topics\n",
    "                if topic_id in topic_names:\n",
    "                    topics_detected.append(f\"[{topic_id}] Topic: {topic_names[topic_id]} - {formatted_percentage}\")\n",
    "                else:\n",
    "                    topics_detected.append(f\"TOPIC {topic_id} - {formatted_percentage}\")\n",
    "            else:\n",
    "                # If bool_title is False, group topics with the same ID and accumulate their probabilities\n",
    "                topics = find_keys_with_value(my_topics, topic_id)\n",
    "                for elem in topics:\n",
    "                    high_topics_detected.setdefault(elem, []).append(percentage)\n",
    "                if len(topics) > 1:\n",
    "                    topics_detected.append(f\"{', '.join(topics).upper()} ({topic_id}) - {formatted_percentage}\")\n",
    "                elif len(topics) == 1:\n",
    "                    topics_detected.append(f\"{topics[0].upper()} ({topic_id}) - {formatted_percentage}\")\n",
    "\n",
    "print(\"Topics detected:\")\n",
    "if bool_auto:\n",
    "    # If bool_auto is True, print the high_topics_detected dictionary sorted by probabilities\n",
    "    high_topics_detected = dict(sorted(high_topics_detected.items(), key=lambda x: x[1], reverse=True))\n",
    "    for key, value in high_topics_detected.items():\n",
    "        if key != 'NA':\n",
    "            p = \"{:.2f}%\".format(value)\n",
    "            print(f\"{key.upper()} - {p}\")\n",
    "else:\n",
    "    sum_dict = {key: sum(lst) for key, lst in high_topics_detected.items()}\n",
    "    if len(topics_detected) == 0:\n",
    "        print('None.')\n",
    "    else:\n",
    "        if bool_group and not bool_title:\n",
    "            # If bool_group is True and bool_title is False, group topics by summing their probabilities\n",
    "            for key, value in sum_dict.items():\n",
    "                p = \"{:.2f}%\".format(value)\n",
    "                print(f\"{key.upper()} - {p}\")\n",
    "        else:\n",
    "            # Print the individual detected topics\n",
    "            for topic in topics_detected:\n",
    "                print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e2fbd1",
   "metadata": {},
   "source": [
    "# SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51bb381",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Path to the original data file\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(os.path.join(path_original_data, 'post_processed_comment_data_demo.csv'), low_memory=False)\n",
    "\n",
    "# List of topics\n",
    "topic_list = [\"complex or complicated\", \"luck\", \"interaction\", \"bash the leader\", \"downtime\", \"bookeeping\"]\n",
    "\n",
    "# Precompute and store the document topics\n",
    "document_topics = [lda.get_document_topics(bow) for bow in bow_corpus]\n",
    "\n",
    "# Define a function to compute topic estimation\n",
    "def topic_estimation(n, input_topic):\n",
    "    # Calculate the sum of topic probabilities for the given input topic\n",
    "    prob_count = sum(topic_prob for topic_id, topic_prob in document_topics[n] if input_topic in topic_classification[topic_id])\n",
    "    return prob_count\n",
    "\n",
    "# Define a function to compute keyword matches\n",
    "def keyword_matches(n):\n",
    "    # Get the bag-of-words representation for the document\n",
    "    document_bow = bow_corpus[n]\n",
    "    document_bow_ids = [id for id, _ in document_bow]\n",
    "    # Convert the word IDs to their corresponding words in the dictionary\n",
    "    document_original = \" \".join([dictionary[id] for id in document_bow_ids])\n",
    "    # Count the number of keywords (words in the dictionary) in the document\n",
    "    return len(document_original.split())\n",
    "\n",
    "# Add a new column 'dictionary_matches' using vectorized operations\n",
    "df['dictionary_matches'] = df.index.map(keyword_matches)\n",
    "\n",
    "# Create new columns for each topic in topic_list using vectorized operations\n",
    "for topic in topic_list:\n",
    "    # Compute the topic estimation for each document\n",
    "    df[topic.replace(' ', '_') + '_estimation'] = [topic_estimation(n, topic) for n in df.index]\n",
    "\n",
    "# Multiply topic estimations with dictionary matches to get relative values\n",
    "for topic in topic_list:\n",
    "    column_name = topic.replace(' ', '_') + '_relative_value'\n",
    "    df[column_name] = df[topic.replace(' ', '_') + '_estimation'] * df['dictionary_matches']\n",
    "\n",
    "# Rename the topic estimation columns\n",
    "df.rename(columns={topic: topic.replace(' ', '_') + '_estimation' for topic in topic_list}, inplace=True)\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "df.to_csv('lpa_comment_data_demo.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626179e7",
   "metadata": {},
   "source": [
    "# READ RESULTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68de8f82",
   "metadata": {},
   "source": [
    "### IMPORTANT VARIABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a99cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC TO SEARCH\n",
    "string = \"luck\"\n",
    "\n",
    "# VARIABLE TO SEARCH\n",
    "value = '_relative_value'\n",
    "# value = '_estimation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4699cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create a topic name by replacing spaces with underscores and appending the value\n",
    "topic = string.replace(' ', '_') + value\n",
    "\n",
    "# Sort the DataFrame based on the values in the specified topic column in descending order\n",
    "df_sorted = df.sort_values(topic, ascending=False)\n",
    "\n",
    "# Extract the 'comment' column of the first five rows\n",
    "comments = df_sorted['comment'].head(5)\n",
    "\n",
    "# Convert the comments to a list\n",
    "comment_list = comments.tolist()\n",
    "\n",
    "# Print each comment in the comment_list\n",
    "for comment in comment_list:\n",
    "    print(comment, '\\n')\n",
    "\n",
    "# Display the first five rows of the sorted DataFrame\n",
    "df_sorted.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c8788",
   "metadata": {},
   "source": [
    "# TRIM DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2cf294",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "\n",
    "# Function to write a list of words to a file\n",
    "def write_words_to_file(word_list, filename):\n",
    "    with open(filename, 'a') as file:\n",
    "        for word in word_list:\n",
    "            file.write(word + '\\n')\n",
    "\n",
    "# Get the topic-word distribution matrix\n",
    "topic_word_matrix = lda.get_topics()\n",
    "\n",
    "# Get the vocabulary from the LDA model\n",
    "vocab = lda.id2word\n",
    "\n",
    "# Create a dictionary to store word counts\n",
    "word_counts = {}\n",
    "\n",
    "# Iterate over each topic\n",
    "for topic_idx, topic_words in enumerate(topic_word_matrix):\n",
    "    \n",
    "    # Sort the word indices based on the word probabilities in descending order\n",
    "    word_indices = topic_words.argsort()[::-1]\n",
    "    \n",
    "    # Iterate over the top 10 words for the topic\n",
    "    for rank, word_idx in enumerate(word_indices[:10]):\n",
    "        word = vocab[word_idx]\n",
    "        word_prob = topic_words[word_idx]\n",
    "        \n",
    "        # Increment the count for the word in the word_counts dictionary\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "        \n",
    "        # Print the rank, word, and probability for each word in the topic\n",
    "        # print(f\"   {rank + 1}. {word}: {word_prob:.4f}\")\n",
    "\n",
    "# Sort the word counts in descending order\n",
    "sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Initialize a list to store words that occur more than twice\n",
    "words_to_remove = []\n",
    "\n",
    "# Iterate over the sorted word counts\n",
    "for word, count in sorted_word_counts:\n",
    "    if count > 2:\n",
    "        # Print the word and its count if it occurs more than twice\n",
    "        print(f'{word}: {count}')\n",
    "        words_to_remove.append(word)\n",
    "\n",
    "# Write the words to be removed to a file\n",
    "write_words_to_file(words_to_remove, 'words_to_remove.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3955508",
   "metadata": {},
   "source": [
    "### ORIGINAL FUNCTION FOR CHECKING RESULTS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5147864c",
   "metadata": {},
   "source": [
    "def extract_comment(n):\n",
    "    target_bow = bow_corpus[n]  # Example target `bow_corpus` element to find its corresponding index in `comments_list`\n",
    "\n",
    "    index = next((i for i, bow_element in enumerate(bow_corpus) if bow_element == target_bow), None)\n",
    "\n",
    "    if index is not None:\n",
    "        print(\"Index of the element in `comments_list`:\", index)\n",
    "    else:\n",
    "        print(\"Element not found in `comments_list`.\")\n",
    "    print()\n",
    "\n",
    "    document_bow = bow_corpus[index]\n",
    "\n",
    "    # Convert the document back to its original form\n",
    "    document_original = \" \".join([dictionary[id] for id, _ in document_bow])\n",
    "\n",
    "    # Print the original document\n",
    "    print(\"Key words in dictionary:\")\n",
    "    print(document_original)\n",
    "    print()\n",
    "\n",
    "    element = comments_list[index]\n",
    "\n",
    "    # Retrieve the corresponding 'comment' field from df\n",
    "    comment = df.loc[index, 'comment']\n",
    "\n",
    "    print(\"Key words from comments_list:\")\n",
    "    print(element)\n",
    "    print()\n",
    "    print(\"Corresponding 'comment' field:\")\n",
    "    print(comment)\n",
    "    print()\n",
    "\n",
    "    return document_bow\n",
    "\n",
    "import random\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import operator\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "random_number = round(random.uniform(0, len(bow_corpus)))\n",
    "\n",
    "document_topics = lda.get_document_topics(extract_comment(random_number))\n",
    "sorted_topics = sorted(document_topics, key=operator.itemgetter(1), reverse=True)\n",
    "\n",
    "print(\"Topics:\")\n",
    "for topic_id, topic_prob in sorted_topics:\n",
    "    if topic_prob > 0.15:\n",
    "        print(f\"[{topic_id}] Topic: {topic_names[topic_id]} - {topic_prob}\")\n",
    "        # print(f\"[{topic_id}] Topic: {topic_prob}\")\n",
    "        for word_id, p in lda.get_topic_terms(topic_id):\n",
    "            word = dictionary[word_id]\n",
    "            \n",
    "            wprob, wlist =  topic_dict[topic_id][word]\n",
    "            \n",
    "            word_list = []\n",
    "            for string in wlist:\n",
    "                words = string.split()\n",
    "                word_list.extend(words)\n",
    "            \n",
    "            closest_match = find_closest_match(word, word_list)\n",
    "            if closest_match is None: closest_match = min(wlist, key=len) + '*'\n",
    "            if closest_match.isupper(): closest_match = '* ' + word_list[0]\n",
    "            print(closest_match, p)\n",
    "        print()    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
