{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc31a56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import ast\n",
    "\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "df = pd.read_csv(os.path.join(path_original_data,'post_processed_comment_data_demo.csv'), low_memory=False)\n",
    "string_list = df['gensim_comment_verbs'].tolist()\n",
    "comments_list = [ast.literal_eval(s) for s in string_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a7002310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_unique_elements(list1, list2):\n",
    "    unique_elements_list1 = list(set(list1) - set(list2))\n",
    "    unique_elements_list2 = list(set(list2) - set(list1))\n",
    "    unique_elements = unique_elements_list1 + unique_elements_list2\n",
    "    return unique_elements\n",
    "\n",
    "def write_words_to_file(word_list, filename):\n",
    "    with open(filename, 'w') as file:\n",
    "        for word in word_list:\n",
    "            file.write(word + '\\n')\n",
    "            \n",
    "def read_words_from_file(filename):\n",
    "    word_list = []\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            word = line.strip()\n",
    "            word_list.append(word)\n",
    "    return word_list            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71fddb1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original size:  34764\n",
      "Trimmed size:  621 \n",
      "\n",
      "Original size:  621\n",
      "Trimmed size:  578 \n",
      "\n",
      "abil\n",
      "abl\n",
      "absolut\n",
      "abstract\n",
      "access\n",
      "action\n",
      "activ\n",
      "actual\n",
      "addit\n",
      "advanc\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "auxiliar = gensim.corpora.Dictionary(comments_list) \n",
    "print('Original size: ',len(auxiliar))\n",
    "list_a = [token for token, idx in auxiliar.token2id.items()]\n",
    "    \n",
    "auxiliar.filter_extremes(no_below=len(auxiliar)*0.01, no_above=0.99, keep_n= None)\n",
    "print('Trimmed size: ',len(auxiliar), '\\n')\n",
    "list_b = [token for token, idx in auxiliar.token2id.items()]\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim import corpora\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "verbs = ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']\n",
    "\n",
    "tokens = [token for token in auxiliar.values()]\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "filtered_tokens = [token for token, pos_tag in zip(tokens, pos_tags) if pos_tag[1] not in verbs]\n",
    "filtered_dictionary = corpora.Dictionary()\n",
    "filtered_dictionary.doc2bow(filtered_tokens, allow_update=True)\n",
    "\n",
    "print('Original size: ',len(auxiliar))\n",
    "print('Trimmed size: ',len(filtered_dictionary), '\\n')\n",
    "list_c = [token for token, idx in filtered_dictionary.token2id.items()]\n",
    "\n",
    "for word in list_c[:10]:\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ace74fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['game', 'player', 'play', 'card', 'like', 'game', 'card', 'great', 'play', 'player'] \n",
      "\n",
      "578\n",
      "572\n"
     ]
    }
   ],
   "source": [
    "filename = 'words_to_remove.txt'\n",
    "words_to_remove = read_words_from_file(filename)\n",
    "\n",
    "print(words_to_remove, '\\n')\n",
    "print(len(filtered_dictionary))\n",
    "\n",
    "word_ids = [filtered_dictionary.token2id[word] for word in words_to_remove if word in filtered_dictionary.token2id]\n",
    "filtered_dictionary.filter_tokens(bad_ids=word_ids)\n",
    "filtered_dictionary.compactify()\n",
    "\n",
    "print(len(filtered_dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9277c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary\n",
    "\n",
    "auxiliar.save('corpus_dictionary')\n",
    "auxiliar = Dictionary.load('corpus_dictionary')\n",
    "# write_words_to_file([token for token, idx in auxiliar.token2id.items()], 'auxiliary_list.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
