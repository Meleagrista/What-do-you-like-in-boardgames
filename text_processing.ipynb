{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1b77b7f",
   "metadata": {},
   "source": [
    "# IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1220bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from requests import Request, Session\n",
    "from requests.exceptions import ConnectionError, Timeout, TooManyRedirects\n",
    "import json\n",
    "import pprint \n",
    "import os\n",
    "\n",
    "# Import progress bar module\n",
    "from alive_progress import alive_bar\n",
    "\n",
    "# Import time and sys modules\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Import BoardGameGeek client\n",
    "from boardgamegeek import BGGClient\n",
    "\n",
    "# Create an instance of the BoardGameGeek client\n",
    "bgg = BGGClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b579a8b",
   "metadata": {},
   "source": [
    "# EXTRACTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2face6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the hot items in the 'boardgame' category from BoardGameGeek\n",
    "hot_items = bgg.hot_items('boardgame')\n",
    "\n",
    "# Create an empty dictionary to store items that encounter errors\n",
    "miss = {}\n",
    "\n",
    "# Create an empty list to store the extracted data\n",
    "data = []\n",
    "\n",
    "# Iterate over each hot item\n",
    "for item in hot_items:\n",
    "    try:\n",
    "        # Retrieve the game details, including comments, for the current item\n",
    "        game = bgg.game(game_id=item.id, comments=True)\n",
    "        \n",
    "        # Create a progress bar with the length of the comments\n",
    "        with alive_bar(len(game.comments), force_tty=True) as bar:\n",
    "            # Iterate over each comment in the game\n",
    "            for comment in game.comments:\n",
    "                # Create a dictionary to store the comment data\n",
    "                com_data = {\n",
    "                    \"id\": item.id,\n",
    "                    \"title\": item.name,\n",
    "                    \"user\": comment.commenter,\n",
    "                    \"comment\": comment.comment,\n",
    "                    \"rating\": comment.rating\n",
    "                }\n",
    "                \n",
    "                # Append the comment data to the list\n",
    "                data.append(com_data)\n",
    "                \n",
    "                # Pause for a short duration to simulate processing time\n",
    "                time.sleep(0.01)\n",
    "                \n",
    "                # Update the progress bar\n",
    "                bar()\n",
    "    except:\n",
    "        # If an error occurs, print 'error' and add the item to the 'miss' dictionary\n",
    "        print('error')\n",
    "        miss[item.id] = item.name\n",
    "\n",
    "# Save the extracted data to a JSON file\n",
    "with open(\"comment_data_demo.json\", 'w') as f:\n",
    "    json.dump(data, f, indent=2)  # indent=2 is not needed but makes the file human-readable if the data is nested   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f565491",
   "metadata": {},
   "source": [
    "# WRANGLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4bc111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set the display option to show all columns in pandas DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Read the JSON file containing the comment data\n",
    "with open(\"comment_data_demo.json\", 'r') as f:\n",
    "    post_list = json.load(f)\n",
    "    \n",
    "# Print the number of comments before any formatting\n",
    "print(f'Amount of comments before any formatting: {len(post_list)}')\n",
    "\n",
    "# Convert the JSON data into a pandas DataFrame\n",
    "df = pd.json_normalize(post_list)\n",
    "\n",
    "# Set the path for the original data directory\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "\n",
    "# Save the DataFrame as a CSV file in the specified directory\n",
    "df.to_csv(os.path.join(path_original_data, 'comment_data_demo.csv'), index=False)\n",
    "\n",
    "# Display the first row of the DataFrame\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f531563",
   "metadata": {},
   "source": [
    "# CLEANING\n",
    "\n",
    "## Reading raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f144b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set the path for the original data directory\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(os.path.join(path_original_data, 'comment_data_demo.csv'), low_memory=False)\n",
    "\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa118e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print some statistics of the 'comment' field\n",
    "\n",
    "# Calculate the percentage of non-null comments\n",
    "comment_percentage = round(df.comment.notnull().mean() * 100, 2)\n",
    "print(str(comment_percentage) + '%')\n",
    "\n",
    "# Calculate the maximum, minimum, and mean length of comments\n",
    "max_length = df.comment.str.len().max()\n",
    "min_length = df.comment.str.len().min()\n",
    "mean_length = df.comment.str.len().mean()\n",
    "print(max_length)\n",
    "print(min_length)\n",
    "print(mean_length)\n",
    "\n",
    "# Search the number of comments containing searched words within the text of the message\n",
    "\n",
    "pattern = \"random\"\n",
    "\n",
    "# Count comments containing the search pattern\n",
    "contains_pattern = df.comment.str.contains(pattern, na=False).sum()\n",
    "\n",
    "# Count comments starting with the search pattern\n",
    "starts_with_pattern = df.comment.str.startswith(pattern, na=False).sum()\n",
    "\n",
    "# Count comments exactly matching the search pattern\n",
    "exact_match_pattern = df.comment.str.fullmatch(pattern, na=False).sum()\n",
    "\n",
    "print(contains_pattern)\n",
    "print(starts_with_pattern)\n",
    "print(exact_match_pattern)\n",
    "\n",
    "# Filter out rows with null comments and reset the index\n",
    "df = df[df.comment.notnull()]\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the first messages that contain the pattern\n",
    "matching_comments = df.loc[df.comment.str.contains(pattern, na=False), 'comment']\n",
    "print(matching_comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b879a8a",
   "metadata": {},
   "source": [
    "## Filtering the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd3c377",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guess_language import guess_language\n",
    "import enchant\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Function to check if a comment is in English\n",
    "def is_english_batch(batch):\n",
    "    # Create a batch of processed texts\n",
    "    processed_texts = batch['comment'].str.lower().str.findall(r\"[a-zA-Z0-9']+\")\n",
    "\n",
    "    # Create an English dictionary\n",
    "    english_dictionary = enchant.Dict(\"en_US\")\n",
    "\n",
    "    # Check if any comment in the batch is in English\n",
    "    is_english = processed_texts.apply(lambda text: sum(english_dictionary.check(word) for word in text) >= len(text) / 2)\n",
    "\n",
    "    # Return a boolean Series indicating if each comment is in English\n",
    "    return is_english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70f1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "from alive_progress import alive_bar\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Batch processing\n",
    "batch_size = 1000  # Number of rows to process in each batch\n",
    "num_rows = len(df)\n",
    "result = pd.Series([], dtype='float64')  # Store the results\n",
    "\n",
    "# Calculate the number of batches\n",
    "num_batches = (num_rows // batch_size) + 1\n",
    "\n",
    "# Initialize a progress bar\n",
    "with tqdm(total=num_batches, ncols=num_batches) as pbar:\n",
    "    # Process each batch\n",
    "    for i in range(0, num_rows, batch_size):\n",
    "        # Extract a batch of rows from the DataFrame\n",
    "        batch = df.iloc[i:i+batch_size]\n",
    "        \n",
    "        # Filter out non-English rows in the batch\n",
    "        batch_english = batch.loc[is_english_batch(batch)]\n",
    "        \n",
    "        # Concatenate the English rows to the result\n",
    "        result = pd.concat([result, batch_english])\n",
    "        \n",
    "        # Update the progress bar\n",
    "        pbar.update(1)\n",
    "\n",
    "# Reset the index of the resulting DataFrame\n",
    "result.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the updated DataFrame\n",
    "result.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9607c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming you have two DataFrames: df1 and df2 representing the two databases\n",
    "df1 = df\n",
    "df2 = result\n",
    "\n",
    "# Print the size of each database\n",
    "print('Current database:', len(result))\n",
    "print('Original database:', len(df))\n",
    "print('Difference:', len(df)-len(result))\n",
    "\n",
    "# Find rows with differing 'comment' in df1 compared to df2\n",
    "diff_df1 = df1[~df1['comment'].isin(df2['comment'])]\n",
    "\n",
    "# Find rows with differing 'comment' in df2 compared to df1\n",
    "diff_df2 = df2[~df2['comment'].isin(df1['comment'])]\n",
    "\n",
    "# Concatenate the differing rows into a single DataFrame\n",
    "diff_combined = pd.concat([diff_df1, diff_df2])\n",
    "\n",
    "# Reset the index of the resulting DataFrame\n",
    "diff_combined.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Print the differences\n",
    "diff_combined.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32685aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column with the length of each comment\n",
    "result['text_length'] = result['comment'].apply(lambda x: len(x))  \n",
    "\n",
    "# Add a new column with the word count of each comment\n",
    "result['word_count'] = result['comment'].apply(lambda x: len(x.split())) \n",
    "\n",
    "# Filter out rows with word count less than or equal to 5\n",
    "result = result[result['word_count'] > 5]  \n",
    "\n",
    "# Drop the first column (assumed to be unnecessary)\n",
    "result = result.drop(result.columns[0], axis=1)  \n",
    "\n",
    "# Print the first 5 rows of the resulting DataFrame\n",
    "result.head(5)  \n",
    "\n",
    "# Save the pre-processed DataFrame to a CSV file\n",
    "result.to_csv('pre_processed_comment_data_demo.csv', index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca7f5b3",
   "metadata": {},
   "source": [
    "# PREPROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ff666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Set the display option to show all columns in pandas DataFrame\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set the path to the original data directory\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "\n",
    "# Construct the file path to the CSV file\n",
    "csv_file_path = os.path.join(path_original_data, 'pre_processed_comment_data_demo.csv')\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(csv_file_path, low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47da6c6",
   "metadata": {},
   "source": [
    "## Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad253b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# Function to remove punctuation from a text\n",
    "def remove_punctuation(text):\n",
    "    # Create a set of allowed characters (letters, numbers, and space)\n",
    "    allowed_chars = set(string.ascii_letters + string.digits + ' ')\n",
    "    \n",
    "    # Remove punctuation characters not in the allowed set\n",
    "    processed_text = ''.join(char for char in text if char in allowed_chars)\n",
    "    \n",
    "    return processed_text\n",
    "\n",
    "# Apply the remove_punctuation() function to the 'comment' column and store the result in a new column 'processed_comment'\n",
    "df['processed_comment'] = df['comment'].apply(remove_punctuation)\n",
    "\n",
    "# Lower case all the messages\n",
    "df['processed_comment'] = df['processed_comment'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e41eff",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3dd8a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to tokenize a text\n",
    "def tokenization(text):\n",
    "    # Split the text on spaces to create tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Apply the tokenization() function to the 'processed_comment' column and store the result in a new column 'comment_tokenized'\n",
    "df['comment_tokenized'] = df['processed_comment'].apply(lambda x: tokenization(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc435a4",
   "metadata": {},
   "source": [
    "## Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dccbe94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "\n",
    "# Download the required NLTK resources (uncomment if needed)\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "np.random.seed(400)\n",
    "\n",
    "# Initialize the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    # Lemmatize each word in the text\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    return lemm_text\n",
    "\n",
    "# Apply the lemmatizer() function to the 'comment_key_words' column and store the result in a new column 'comment_lemmatized'\n",
    "df['comment_lemmatized'] = df['comment_key_words'].apply(lambda x: lemmatizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf6615e",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e1e1aa5c",
   "metadata": {},
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Defining the object for stemming\n",
    "porter_stemmer = PorterStemmer()\n",
    "\n",
    "# Defining a function for stemming\n",
    "def stemming(text):\n",
    "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
    "\n",
    "    return stem_text\n",
    "\n",
    "df['comment_stemmed']= df['comment_key_words'].apply(lambda x: stemming(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174db225",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1fff780",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "import nltk\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Defining the object for Lemmatization\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Defining the function for lemmatization\n",
    "def lemmatizer(text):\n",
    "    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "    \n",
    "    return lemm_text\n",
    "\n",
    "df['comment_lemmatized']= df['comment_key_words'].apply(lambda x:lemmatizer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23db142c",
   "metadata": {},
   "source": [
    "## Gensim preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae5839c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "np.random.seed(400)\n",
    "\n",
    "# Download the required NLTK resource (uncomment if needed)\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "reference_sheet = {}  # Dictionary to store word reference sheet\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# Function to lemmatize and stem a word\n",
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize, lemmatize, and filter stopwords\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in STOPWORDS and len(token) > 2:\n",
    "            word = lemmatize_stemming(token)\n",
    "            if word in reference_sheet:\n",
    "                if token not in reference_sheet[word]:\n",
    "                    reference_sheet[word].append(token)\n",
    "            else:\n",
    "                reference_sheet[word] = [token]\n",
    "            result.append(word)\n",
    "    return result\n",
    "\n",
    "# Tokenize, lemmatize, and filter verbs\n",
    "def preprocess_verbs(text):\n",
    "    text = gensim.utils.simple_preprocess(text)\n",
    "    tagged_tokens = nltk.pos_tag(text)\n",
    "    filtered_tokens = [token for token, pos_tag in tagged_tokens if pos_tag not in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']]\n",
    "    \n",
    "    result = []\n",
    "    for token in filtered_tokens:\n",
    "        if token not in STOPWORDS and len(token) > 2:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result\n",
    "\n",
    "print('Start.')\n",
    "df['gensim_comment'] = df['comment'].apply(preprocess)\n",
    "print('Next.')\n",
    "df['gensim_comment_verbs'] = df['comment'].apply(preprocess_verbs)\n",
    "print('Finish.')\n",
    "\n",
    "# Save reference sheet as a JSON file\n",
    "json_data = json.dumps(reference_sheet)\n",
    "with open('reference_sheet.json', 'w') as file:\n",
    "    file.write(json_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadb7aae",
   "metadata": {},
   "source": [
    "## Restructring the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78769784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of column names\n",
    "columns = list(df.columns)\n",
    "print(columns)\n",
    "\n",
    "# Filter the DataFrame based on the length of 'gensim_comment' column\n",
    "df = df[df['gensim_comment'].map(lambda d: len(d)) >= 5]\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Calculate the average length of 'gensim_comment' column\n",
    "average_length = df['gensim_comment'].apply(lambda x: len(x)).mean()\n",
    "print(average_length)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('post_processed_comment_data_demo.csv', index=False)\n",
    "\n",
    "# Display a sample of 5 rows from the DataFrame\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1836cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set the maximum number of columns to display\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Set the path to the original data file\n",
    "path_original_data = r'C:\\Users\\Usuario\\Documents\\JupyterFolder\\unimi_files\\IR'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv(os.path.join(path_original_data, 'post_processed_comment_data_demo.csv'), low_memory=False)\n",
    "\n",
    "# Display a sample of 10 rows from the DataFrame\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9919f5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the number of posts that contain specific words\n",
    "print(len(df[df.comment.str.contains('luck')]))\n",
    "print(len(df[df.comment.str.contains('random')]))\n",
    "print(len(df[df.comment.str.contains('boring')]))\n",
    "print(len(df[df.comment.str.contains('complex')]))\n",
    "print(len(df[df.comment.str.contains('complicated')]))\n",
    "print(len(df[df.comment.str.contains('bookkeeping')]))\n",
    "\n",
    "print(len(df[df.comment.str.contains('edition')]))\n",
    "print(len(df[df.comment.str.contains('version')]))\n",
    "print(len(df[df.comment.str.contains('expansion')]))\n",
    "\n",
    "# Display a sample of 5 rows from the DataFrame that contain the word 'boring'\n",
    "df[df.comment.str.contains('boring')].sample(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
